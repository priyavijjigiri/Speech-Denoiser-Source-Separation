{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(4444)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "sess = tf.InteractiveSession(config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "files = np.asarray(librosa.util.find_files('/N/u/priyadarshini/Projects/Noise_signals/tr'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Magnitude Spectrograms for all Train signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Noise Signals\n",
    "N = []\n",
    "for noise_signal in files[0:1200]:\n",
    "    sn, sr = librosa.load(noise_signal, sr=None)\n",
    "    Sn = librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    S_noise = np.transpose(np.abs(Sn))\n",
    "    N.append(S_noise)\n",
    "\n",
    "# Clean Speech Signals    \n",
    "S = []\n",
    "for clean_signal in files[1200:2400]:\n",
    "    ss, sr = librosa.load(clean_signal, sr=None)\n",
    "    Ss = librosa.stft(ss, n_fft=1024, hop_length=512)\n",
    "    S_clean = np.transpose(np.abs(Ss))\n",
    "    S.append(S_clean)    \n",
    "\n",
    "\n",
    "# Mixed Noisy Speech Signals    \n",
    "X = []\n",
    "for mix_signal in files[2400:3600]:\n",
    "    sx, sr = librosa.load(mix_signal, sr=None)\n",
    "    Sx = librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "    S_mix = np.transpose(np.abs(Sx))\n",
    "    X.append(S_mix)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating IBM Matrix per spectogram - Training signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IBM = X\n",
    "for k in range(0,len(IBM)) :\n",
    "    for i in range(0,len(IBM[k])):\n",
    "        for j in range(0,len(IBM[k][i])):\n",
    "            if S[k][i][j] > N[k][i][j]:\n",
    "                IBM[k][i][j] = 1\n",
    "            else:\n",
    "                IBM[k][i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for spectogram in X :\n",
    "    rows = len(spectogram)\n",
    "    if rows >max:\n",
    "        max = rows\n",
    "print(max)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding train matrices with zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_m = [np.zeros((178,513))]*1200\n",
    "for k in range(0,len(X)) :\n",
    "    for i in range(0,len(X[k])):\n",
    "        for j in range(0,len(X[k][i])):\n",
    "            X_m[k][i][j] = X[k][i][j]\n",
    "\n",
    "IBM_m = [np.zeros((178,513))]*1200\n",
    "for k in range(0,len(IBM)) :\n",
    "    for i in range(0,len(IBM[k])):\n",
    "        for j in range(0,len(IBM[k][i])):\n",
    "            IBM_m[k][i][j] = IBM[k][i][j]            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42720, 5, 513)\n",
      "(42720, 5, 513)\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array(X_m).reshape(-1,5,513)  \n",
    "IBM_new = np.array(IBM_m).reshape(-1,5,513)\n",
    "print(X_new.shape)\n",
    "print(IBM_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for the RNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 513\n",
    "\n",
    "num_time_steps = 10\n",
    "\n",
    "num_neurons = 128\n",
    "\n",
    "num_outputs = 513\n",
    "\n",
    "learning_rate = 0.001 \n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, None, num_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, None, num_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining RNN network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "    tf.contrib.rnn.BasicLSTMCell(num_units=num_neurons, activation=tf.nn.sigmoid),\n",
    "    output_size=num_outputs) \n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = 0.99)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32);\n",
    "loss = tf.reduce_mean(tf.square(outputs - y)) # MSE\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batching_1(X, IBM, batch_size):\n",
    "        for start_index in range(0, X.shape[0] - batch_size + 1, batch_size):    \n",
    "            index = slice(start_index, start_index + batch_size)\n",
    "            yield X[index], IBM[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNN Network - Acheived train MSE loss of 0.0038 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tLoss: 0.016333217\n",
      "1 \tLoss: 0.0067119976\n",
      "2 \tLoss: 0.004720248\n",
      "3 \tLoss: 0.0038714337\n",
      "4 \tLoss: 0.0038870147\n",
      "5 \tLoss: 0.004064526\n",
      "6 \tLoss: 0.0037911918\n",
      "7 \tLoss: 0.0040458255\n",
      "8 \tLoss: 0.0037615318\n",
      "9 \tLoss: 0.0037018957\n",
      "10 \tLoss: 0.003832265\n",
      "11 \tLoss: 0.003865591\n",
      "12 \tLoss: 0.0038263637\n",
      "13 \tLoss: 0.0038724015\n",
      "14 \tLoss: 0.0038652164\n",
      "15 \tLoss: 0.0039139353\n",
      "16 \tLoss: 0.0037297388\n",
      "17 \tLoss: 0.003745436\n",
      "18 \tLoss: 0.003848712\n",
      "19 \tLoss: 0.003852201\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(0,num_epochs):\n",
    "     for batch in batching_1(X_new, IBM_new, batch_size):\n",
    "        X_batch, y_batch = batch\n",
    "        sess.run(train, feed_dict={x: X_batch, y: y_batch})\n",
    "        \n",
    "     if i % 1 == 0:\n",
    "            \n",
    "            acc = loss.eval(feed_dict={x: X_batch, y: y_batch})\n",
    "            print(i, \"\\tLoss:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set - Creating Magnitude Spectrograms for all signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_v = np.asarray(librosa.util.find_files('/N/u/priyadarshini/Projects/Noise_signals/v'))\n",
    "\n",
    "# Noise Signals\n",
    "Nv = []\n",
    "for noise_signal in files_v[0:1200]:\n",
    "    snv, sr = librosa.load(noise_signal, sr=None)\n",
    "    #Snv = librosa.stft(snv, n_fft=1024, hop_length=512)\n",
    "    #Sn_noise = np.transpose(np.abs(librosa.stft(snv, n_fft=1024, hop_length=512)))\n",
    "    Nv.append(np.transpose(np.abs(librosa.stft(snv, n_fft=1024, hop_length=512))))\n",
    "\n",
    "# Clean Speech Signals    \n",
    "Sv = []\n",
    "for clean_signal in files_v[1200:2400]:\n",
    "    ssv, sr = librosa.load(clean_signal, sr=None)\n",
    "    #Ssv = librosa.stft(ssv, n_fft=1024, hop_length=512)\n",
    "    #Sv_clean = np.transpose(np.abs(librosa.stft(ssv, n_fft=1024, hop_length=512)))\n",
    "    Sv.append(np.transpose(np.abs(librosa.stft(ssv, n_fft=1024, hop_length=512))))    \n",
    "\n",
    "\n",
    "# Mixed Noisy Speech Signals    \n",
    "Xv = []\n",
    "for mix_signal in files_v[2400:3600]:\n",
    "    sxv, sr = librosa.load(mix_signal, sr=None)\n",
    "    #Sxv = librosa.stft(sxv, n_fft=1024, hop_length=512)\n",
    "    #Sv_mix = np.transpose(np.abs(librosa.stft(sxv, n_fft=1024, hop_length=512)))\n",
    "    Xv.append(np.transpose(np.abs(librosa.stft(sxv, n_fft=1024, hop_length=512))))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating IBM matrix for validation spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IBMv = Xv\n",
    "for k in range(0,len(IBMv)) :\n",
    "    for i in range(0,len(IBMv[k])):\n",
    "        for j in range(0,len(IBMv[k][i])):\n",
    "            if Sv[k][i][j] > Nv[k][i][j]:\n",
    "                IBMv[k][i][j] = 1\n",
    "            else:\n",
    "                IBMv[k][i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "max1 = 0\n",
    "for spectogram in Xv :\n",
    "    rows = len(spectogram)\n",
    "    if rows >max1:\n",
    "        max1 = rows\n",
    "print(max1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Validation matrices with zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_mv = [np.zeros((178,513))]*1200\n",
    "for k in range(0,len(Xv)) :\n",
    "    for i in range(0,len(Xv[k])):\n",
    "        for j in range(0,len(Xv[k][i])):\n",
    "            X_mv[k][i][j] = Xv[k][i][j]\n",
    "            \n",
    "IBM_mv = [np.zeros((178,513))]*1200\n",
    "for k in range(0,len(IBMv)) :\n",
    "    for i in range(0,len(IBMv[k])):\n",
    "        for j in range(0,len(IBMv[k][i])):\n",
    "            IBM_mv[k][i][j] = IBMv[k][i][j]   \n",
    "            \n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21360, 10, 513)\n",
      "(21360, 10, 513)\n"
     ]
    }
   ],
   "source": [
    "X_new_v = np.array(X_mv).reshape(-1,10,513)  \n",
    "IBM_new_v = np.array(IBM_mv).reshape(-1,10,513)\n",
    "\n",
    "print(X_new_v.shape)\n",
    "print(IBM_new_v.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Acheived MSE Loss of 0.055  on Validation signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055380636205275856\n"
     ]
    }
   ],
   "source": [
    "mse_v = 0\n",
    "i = 0\n",
    "for batch in batching_1(X_new_v, IBM_new_v, num_time_steps):\n",
    "        i = i + 1\n",
    "        X_batch_v, y_batch_v = batch\n",
    "        mse_v = mse_v + loss.eval(feed_dict={x: X_batch_v, y: y_batch_v})\n",
    "        \n",
    "mse_v = mse_v/i        \n",
    "print(mse_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Test Signals - Creating spectograms for files in testing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_t = np.asarray(librosa.util.find_files('/N/u/priyadarshini/Projects/Noise_signals/te'))\n",
    "\n",
    "# Mixed Noisy Speech Signals    \n",
    "Xt = []\n",
    "St = []\n",
    "\n",
    "for mix_signal in files_t[0:400]:\n",
    "    sxt, sr = librosa.load(mix_signal, sr=None)\n",
    "    Sxt = librosa.stft(sxt, n_fft=1024, hop_length=512)\n",
    "    St.append(Sxt)\n",
    "    Xt.append(np.transpose(np.abs(Sxt)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Producing Clean Test signals and Writing to the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(files_t)):\n",
    "    s, sr = librosa.load(files_t[i], sr=None)\n",
    "    test_c = librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "    test_abs = np.transpose(np.abs(test_c))\n",
    "    test_abs = test_abs.reshape(1, -1, 513)\n",
    "    test_denoise_trans = sess.run(outputs, feed_dict={x: test_abs})\n",
    "    test_denoise_reshaped = test_denoise_trans.reshape(-1, 513) \n",
    "    test_denoise = np.transpose(test_denoise_reshaped)\n",
    "    test_phase = np.divide(test_c, np.abs(test_c))\n",
    "    test_denoise_phase = np.multiply(test_denoise, test_phase)\n",
    "    sp = librosa.istft(test_denoise_phase,hop_length=512)\n",
    "    librosa.output.write_wav('/N/u/priyadarshini/Projects/Test Clean Signals/tex_s_recons_' + str(i) + '.wav', sp, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipping the Denoised test signals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile(\"myzipfile.zip\", \"w\")\n",
    "for dirname, subdirs, files in os.walk(\"/N/u/priyadarshini/Projects/Test Clean Signals\"):\n",
    "    zf.write(dirname)\n",
    "    for filename in files:\n",
    "        zf.write(os.path.join(dirname, filename))\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
